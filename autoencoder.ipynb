{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBy4JFOlc9Hr"
      },
      "source": [
        "In this assignment, we'll explore a more advanced use of deep learning on a\n",
        "natural language process (NLP) task involving news headlines. \n",
        "In particular, we'll be working with a dataset of Reuters news headlines\n",
        "collected over a span of 15 months, covering some of 2018, 2019, and early 2020.\n",
        "This assignment will combine several of the concepts that we discussed in class,\n",
        "including recurrent neural networks, data augmentation, autoencoders (soon), \n",
        "and working with embeddings.\n",
        "\n",
        "To be more specific, we'll be building an **autoencoder** of news headlines.\n",
        "We will build an **encoder** model that maps a news headline to a vector embedding,\n",
        "and a **decoder** that reconstructs the news headline. By building a model that\n",
        "learns to reconstruct the news headlines from the vector embedding, the model\n",
        "will learn good embeddings of these headlins.\n",
        "\n",
        "We'll see a similar idea with image autoencoders and image VAEs, but\n",
        "both our encoder and decoder networks will be Recurrent Neural Networks.\n",
        "You'll have a chance build networks that takes a sequence as an input,\n",
        "and a network that generates a sequence as an output.\n",
        "\n",
        "This project is organized as follows:\n",
        "\n",
        "- Part 1. Data exploration\n",
        "- Part 2. Background Math\n",
        "- Part 3. Building the autoencoder\n",
        "- Part 4. Training the autoencoder using *data augmentation*\n",
        "- Part 5. Analyzing the embeddings (interpolating between headlines)\n",
        "\n",
        "Much of the idea behind this assignment is movtivated by Shen et al [1].\n",
        "We'll use the data augmentation rules proposed in that work to improve\n",
        "the robustness of the autoencoder.\n",
        "\n",
        "[1] Shen et al (2019) \"Educating Text Autoencoders: Latent Representation Guidance via Denoising\" https://arxiv.org/pdf/1905.12777.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8STLp0gPc9H1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BT8z0qhuHRqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkJJMvRc9H3"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Download the files `reuters_train.txt` and `reuters_valid.txt`, and upload them to Google Drive.\n",
        "\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eay8WoXPc9H4",
        "outputId": "71b7c32c-cc4b-4862-f458-e6c72294b18a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "train_path = '/content/gdrive/My Drive/reuters_train.txt' \n",
        "valid_path = '/content/gdrive/My Drive/reuters_valid.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byuGbjcfc9H6"
      },
      "source": [
        "We will be using PyTorch's `torchtext` utilities to help us load, process,\n",
        "and batch the data. This package is useful, but takes a bit of time to get\n",
        "used to. \n",
        "\n",
        "We'll be using a `TabularDataset` to load our data, which works well on structured\n",
        "CSV data with fixed columns (e.g. a column for the sequence, a column for the label). Our tabular dataset\n",
        "is even simpler: we have no labels, just some text. So, we are treating our data as a table with one field\n",
        "representing our sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RynnXQG7c9H8"
      },
      "source": [
        "import torchtext.legacy as torchtext\n",
        "# Tokenization function to separate a headline into words\n",
        "def tokenize_headline(headline):\n",
        "    \"\"\"Returns the sequence of words in the string headline. We also\n",
        "    prepend the \"<bos>\" or beginning-of-string token, and append the\n",
        "    \"<eos>\" or end-of-string token to the headline.\n",
        "    \"\"\"\n",
        "    return (\"<bos> \" + headline + \" <eos>\").split()\n",
        "    \n",
        "# Data field (column) representing our *text*.\n",
        "text_field = torchtext.data.Field(\n",
        "    sequential=True,            # this field consists of a sequence\n",
        "    tokenize=tokenize_headline, # how to split sequences into words\n",
        "    include_lengths=True,       # to track the length of sequences, for batching\n",
        "    batch_first=True,      # similar to batch_first=True in nn.RNN demonstrated in lecture\n",
        "    use_vocab=True)             # to turn each character into an integer index\n",
        "train_data = torchtext.data.TabularDataset(\n",
        "    path=train_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (we have only one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Q0b_TDc9H9"
      },
      "source": [
        "Draw histograms of the number of words per headline in our training set.\n",
        "Excluding the `<bos>` and `<eos>` tags in your computation.\n",
        "Explain why we would be interested in such histograms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "AEV3vYSmc9H-",
        "outputId": "402de636-a7b1-465d-aff9-5cd222986866"
      },
      "source": [
        "histogram_data = []\n",
        "for article in train_data:\n",
        "  histogram_data.append(len(article.title))\n",
        "number_per_headline = np.array(histogram_data) - 2\n",
        "total_words = np.sum(number_per_headline) # Total words in all headlines used in 1(d)\n",
        "\n",
        "print(number_per_headline.shape)\n",
        "plt.hist(number_per_headline)\n",
        "# Number of words per headline effects the attention aspect of the RNN. More words mean \n",
        "# more weights will be included in between layers. We are interested to know about these\n",
        "# to optimize our model better."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(171443,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.0900e+02, 9.0940e+03, 6.1255e+04, 5.6220e+04, 3.8670e+04,\n",
              "        5.4620e+03, 3.0400e+02, 2.3000e+01, 5.0000e+00, 1.0000e+00]),\n",
              " array([ 2. ,  4.7,  7.4, 10.1, 12.8, 15.5, 18.2, 20.9, 23.6, 26.3, 29. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxZsRaDHc9H-"
      },
      "source": [
        "Exclude the `<bos>` and `<eos>` tags in your computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9MAXrtXc9H-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842ee0ab-4051-4d29-c162-273bbe3622bc"
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        "\n",
        "# You might find the python class Counter from the collections package useful\n",
        "from collections import Counter\n",
        "all_words = []\n",
        "\n",
        "for article in train_data:\n",
        "  for words in article.title:\n",
        "    if words != '<bos>' and words != '<eos>':\n",
        "      all_words.append(words)\n",
        "\n",
        "counter_words = Counter(all_words)\n",
        "print(\"Number of distinct words:\")\n",
        "print(len(counter_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct words:\n",
            "51298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuV40diRc9H_"
      },
      "source": [
        "The distribution of *words* will have a long tail, meaning that there are some words\n",
        "that will appear very often, and many words that will appear infrequently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExCMQViVc9IA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab3c7d1-d8fc-47b5-af19-3b50d5bf67fd"
      },
      "source": [
        "count_1 = 0\n",
        "count_2 = 0\n",
        "for frequency in counter_words.values():\n",
        "  if frequency == 1:\n",
        "    count_1 += 1\n",
        "  if frequency == 2:\n",
        "    count_2 += 1\n",
        "\n",
        "print(\"Exactly once:\")\n",
        "print(count_1)\n",
        "print(\"Exactly twice:\")\n",
        "print(count_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exactly once:\n",
            "19854\n",
            "Exactly twice:\n",
            "7193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cDziEt3c9IB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmQ0Xs_hc9IB"
      },
      "source": [
        "# There are few reasons:\n",
        "# 1) It is faster to train if our model works with less words\n",
        "# 2) There will always be rare words or even mistyped words that is not in our vocabulary,\n",
        "# our model should be able to make predictions without knowing every single word.\n",
        "# 3) Knowing every single word in our training data could overfit the model to it, model\n",
        "# would start predicting wrong when dealing with words outside of the training data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlEy_UmCc9IC"
      },
      "source": [
        "We will only model the top 9995 words in the training set, excluding the tags\n",
        "`<bos>`, `<eos>`, and other possible tags we haven't mentioned yet\n",
        "(including those, we will have a vocabulary size of exactly 10000 tokens)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbqA4tnMc9IC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25d55c0-056b-4182-b7b9-af032e9e0d71"
      },
      "source": [
        "supported_occurences = 0\n",
        "counter = 0\n",
        "for keys in counter_words.keys():\n",
        "  supported_occurences += counter_words[keys]\n",
        "  counter += 1\n",
        "  if counter == 9995:\n",
        "    break\n",
        "\n",
        "sup = supported_occurences/total_words\n",
        "not_sup = 1 - sup\n",
        "print(\"Percentage of supported words:\")\n",
        "print(sup)\n",
        "print(\"Percentage of `<unk>` tag words:\")\n",
        "print(not_sup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of supported words:\n",
            "0.890645389865422\n",
            "Percentage of `<unk>` tag words:\n",
            "0.10935461013457803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AGwUldxc9IC"
      },
      "source": [
        "Our `torchtext` package will help us keep track of our list of unique words, known\n",
        "as a **vocabulary**. A vocabulary also assigns a unique integer index to each word.\n",
        "You can interpret these indices as sparse representations of one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjAD0vNIc9ID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746d2675-3c0f-4554-d46c-6c88453b8ddf"
      },
      "source": [
        "# Build the vocabulary based on the training data. The vocabulary\n",
        "# can have at most 9997 words (9995 words + the <bos> and <eos> token)\n",
        "text_field.build_vocab(train_data, max_size=9997)\n",
        "\n",
        "# This vocabulary object will be helpful for us\n",
        "vocab = text_field.vocab\n",
        "print(vocab.stoi[\"hello\"]) # for instances, we can convert from string to (unique) index\n",
        "print(vocab.itos[10])      # ... and from word index to string\n",
        "\n",
        "# The size of our vocabulary is actually 10000\n",
        "vocab_size = len(text_field.vocab.stoi)\n",
        "print(vocab_size) # should be 10000\n",
        "\n",
        "# The reason is that torchtext adds two more tokens for us:\n",
        "print(vocab.itos[0]) # <unk> represents an unknown word not in our vocabulary\n",
        "print(vocab.itos[1]) # <pad> will be used to pad short sequences for batching"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "on\n",
            "10000\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k9ZyBaANrjN"
      },
      "source": [
        "Choosing the right model architecture is key for any successful deep learning system.\n",
        "In this question, we will compare the learning performance of RNNs and GRUs from the perspective\n",
        "of the vanishing/exploding gradient problem that arises during backpropagation.\n",
        "\n",
        "First, we will analyze the recurrent weight matrix of an RNN using Singular Value Decomposition (SVD). SVD says that that any real matrix $M \\in \\mathbb{R}^{m \\times n}$ can be written as $W = U \\Sigma V^T$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are square orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix. Recall that the values of $\\Sigma$ are the *eigenvalues* of $M$.o\n",
        "\n",
        "(For a quick overview of eigenvalues and eigenvectors, see https://www.youtube.com/watch?v=PFDu9oVAE-g 0:44-5:22 and 13:05-end. The last section\n",
        "explains visually why we are interested in working with eigenvalues when working with an RNN)\n",
        "\n",
        "Consider a simple simple RNN-like architecture that computes $x_{t+1} = sigmoid(W x_t)$. You can\n",
        "view this architecture as a deep fully connected network that uses the same weight matrix at each\n",
        "layer. Suppose the largest singular value of the weight matrix is $\\sigma_{max}(W) = \\frac{1}{2}$. \n",
        "\n",
        "Show that the largest singular value of the input-output Jacobian has the following bound: $0 \\leq \\sigma_{max}(\\frac{\\partial x_n}{\\partial x_1}) \\leq (\\frac{1}{2})^n$. $\\textit{Hint}$: if $C = AB$, then $\\sigma_{max}(C) \\le \\sigma_{max}(A) \\sigma_{max}(B)$. Also, the input-output Jacobian is the multiplication of layerwise Jacobians).\n",
        "\n",
        "What does this tell us about the input-output Jacobian $\\frac{\\partial x_n}{\\partial x_1}$ as $n \\rightarrow \\infty$ ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR-V--l6NzMv"
      },
      "source": [
        "\\begin{align*}\n",
        "{P}({n}) &: 0 \\leq \\sigma_{max}(\\frac{\\partial x_n}{\\partial x_1}) \\leq (\\frac{1}{2})^n \\\\\n",
        "\\text{Base case} &: \\text{$n=1$ $x_1=x_{0+1}$} = \\sigma(Wx_0)  \\\\\n",
        "\\frac{\\partial x_1}{\\partial x_1} &= 1\\\\\n",
        "\\text{Which clearly fails}\n",
        "\\\\\n",
        "\\text{$n=2$ $x_2=x_{1+1}$} = \\sigma(Wx_1)  \\\\\n",
        "\\frac{\\partial x_2}{\\partial x_1} &= \\frac{\\partial }{\\partial x_1} (\\sigma(Wx_1))\\\\\n",
        "&= W\\sigma'(x_1)\\\\\n",
        "\\text{We know that} &: \\sigma_{max}(W)=\\frac{1}{2} \\text{ and } \\sigma_{max}(\\sigma'(x_1))\n",
        "\\end{align*}\n",
        "\\begin{align*}\n",
        "=\\frac{1}{4}\\\\ \\because \\text{Maximmum of derivative of sigmoid is 0.25 as proven in previous assignments.}\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Hence } \\sigma_{max}(W\\sigma'(x_1)) &=  \\sigma_{max}(W)\\sigma_{max}(\\sigma'(x_1))\\\\\n",
        "&= \\frac{1}{2} a\\\\\n",
        "\\text{Where}, 0 \\leq &a \\leq \\frac{1}{4}\\\\\n",
        "&\\therefore 0 \\leq \\sigma_{max}(\\frac{\\partial x_2}{\\partial x_1}) \\leq (\\frac{1}{2})^2 : \\text{ is True}\\\\\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Induction hypothesis} &: \\text{Assume the following holds all} \\\\\n",
        "k &< n \\\\\n",
        "\\text{i.e } 0 &\\leq \\sigma_{max}(\\frac{\\partial x_k}{\\partial x_1}) \\leq (\\frac{1}{2})^k \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Induction step} &: \\text{for $k+1$} \\\\\n",
        "\\sigma_{max}(\\frac{\\partial x_{k+1}}{\\partial x_1}) &= \\sigma_{max}((\\frac{\\partial x_{k+1}}{\\partial x_k})(\\frac{\\partial x_{k}}{\\partial x_1})) \\because \\text{Chain rule} \\\\\n",
        "&= \\sigma_{max}(\\frac{\\partial x_{k+1}}{\\partial x_k})\\sigma_{max}(\\frac{\\partial x_{k}}{\\partial x_1}) \\\\\n",
        "&= \\sigma_{max}(\\frac{\\partial \\sigma(Wx_n)}{\\partial x_n})b \\text{ where $b \\leq (\\frac{1}{2})^k$, induction hypothesis} \\\\\n",
        "&= \\sigma_{max}(W\\sigma'(x_n))b \\\\\n",
        "&= \\sigma_{max}(W)\\sigma_{max}(\\sigma'(x_n))b \\\\\n",
        "&= \\frac{1}{2} c b , \\text{where }  0 \\leq c \\leq \\frac{1}{4} \\\\\n",
        "&= \\frac{1}{2} c b \\leq (\\frac{1}{2})^{k+1} \n",
        "\\\\\n",
        "\\text{Hence} &: 0 \\leq \\sigma_{max}(\\frac{\\partial x_{k+1}}{\\partial x_1}) \\leq(\\frac{1}{2})^{k+1} \\\\\n",
        "\\text{Base case 2 is true. } & \\text{ $P(k)$ is true then $P(k+1)$ is true} \\\\\n",
        "&\\therefore \\text{From mathematical induction $P(n)$ is true.} \\\\\n",
        "\\text{Hence} &: 0 \\leq \\sigma_{max}(\\frac{\\partial x_n}{\\partial x_1}) \\leq(\\frac{1}{2})^{n}, 2 \\geq n\\\\\n",
        "\\text{Given} &, 0 \\leq \\sigma_{max}(\\frac{\\partial x_n}{\\partial x_1}) \\leq(\\frac{1}{2})^{n}\\\\\n",
        "\\text{then} &: \\lim_{n\\to\\infty} 0 \\leq \\lim_{n\\to\\infty}\\sigma_{max}(\\frac{\\partial x_n}{\\partial x_1}) \\leq \\lim_{n\\to\\infty} (\\frac{1}{2})^{n}\\\\\n",
        "&=  0 \\leq \\lim_{n\\to\\infty}(\\frac{\\partial x_n}{\\partial x_1}) \\leq 0\\\\\n",
        "&=> \\lim_{n\\to\\infty}(\\frac{\\partial x_n}{\\partial x_1}) = 0\\\\\n",
        "&\\therefore \\text{The value would be 0 and the gradient would vanish.}\\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xm1A77zc9IE"
      },
      "source": [
        "We will now compare the gradients of a vanilla RNN unit and a Gated Recurrent Unit (GRU).\n",
        "For both parts (b) and (c), assume that all weights are scalars, and that we have\n",
        "an input sequence of length $T$ with $x_1 = 1$ and $x_t = 0$ for all other $t$.\n",
        "Also, assume that $h_0 = 0$ and that after $T$ timesteps, we calculate the squared\n",
        "loss $L = \\frac{1}{2}(y_T - o_T)^2$ where $o_T$ is the target at timestep $T$.\n",
        "\n",
        "Consider the vanilla RNN units that compute $h_t$ at each timestep $t$ as follows:\n",
        "\n",
        "$$m_t = W_x x_t + W_h h_{t-1}$$\n",
        "$$h_t = \\tanh(m_t)$$\n",
        "$$y_t = W_y h_t$$\n",
        "\n",
        "\n",
        "Compute $\\frac{\\partial L}{\\partial W_x}$ using backpropagation. You should obtain an expression\n",
        "in terms of the quantities given (like $o_T$, $y_T$, etc...)\n",
        "\n",
        "Do you see a vanishing gradient problem? What about exploding gradient? Explain.\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y_T} = y_T - o_T$$\n",
        "$$\\frac{\\partial y_T}{\\partial h_T} = W_y$$\n",
        "$$ \\frac{\\partial h_T}{\\partial m_T} = sech^2(m_T)$$\n",
        "Note that for $m_T$ we can expand it to a recursively instead of using chainrule, $$m_T = W_x x_T + W_h h_{T - 1} = W_x x_T + W_h(tanh(m_{T-1}))$$\n",
        "Eventually, $$m_T = W_x x_T + W_h h_{T - 1} = W_h(tanh(...tanh(W_x)) \\because x_t = 0, x_1 = 1$$\n",
        "$$\\frac{\\partial m_T}{\\partial W_x} = W_h (sech^2(W_x)sech^2(tanh(W_x))sech^2$$\n",
        "$$*(tanh(tanh(W_x)))...sech^2(...(...(tanh(W_x))...)...)$$\n",
        "$$\\frac{\\partial L}{\\partial W_x =} = (y_T - o_t)(W_y)(sech^2(m_T))(W_h(sech^2(W_x)sech^2$$\n",
        "$$*(tanh(W_x)...sech^2(...(...(tanh(W_x))...)...)$$\n",
        "\n",
        "Notice for the final term in the derivative the maximum value of it depends is \n",
        "1 (Look at graph in appendix) which occurs at 0. Hence as $W_x$\n",
        "tends to increase the gradient for it will aproach 0 and vanish. \n",
        "\n",
        "Now, let's consider GRU units that uses a gating mechanism, and computes\n",
        "$h_t$ at each timestep $t$ as follows:\n",
        "\n",
        "$$z_t = \\sigma(W_x x_t + U_z h_{t-1})$$\n",
        "$$r_t = \\sigma(W_r x_t + U_r h_{t-1})$$\n",
        "$$\\hat{h}_t = \\tanh(W_h x_t + U_h r_t h_{t-1})$$\n",
        "$$h_t = (1 - z_t) h_{t-1} + z_t \\hat{h}_t$$\n",
        "$$y_t = W_y h_t$$\n",
        "\n",
        "Where $\\sigma$ is the sigmoid function.\n",
        "\n",
        "Compute $\\frac{\\partial L}{\\partial W_x}$ using backpropagation.\n",
        "\n",
        "Can the vanishing gradient problem be prevented? *Hint* : Consider the term $\\frac{\\partial h_t}{\\partial h_{t-1}}$, what role does $z_t$ play in this gradient? Can it help alleviate the vanishing gradient problem?\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial y_T} = y_T - o_T$$\n",
        "$$\\frac{\\partial y_T}{\\partial h_T} = W_y$$\n",
        "$$\\frac{\\partial h_T}{\\partial {z_T}} = h_{T-1} + \\hat{h_T}$$\n",
        "$$\\frac{\\partial z_T}{\\partial W_x} = \\sigma'(W_xx_T + U_zh_{T-1})$$\n",
        "$$\\Sigma_{i=1}^T\\frac{\\partial h_T}{\\partial {h_{T-i}}} = \\Sigma_{i=1}^T\\Pi_{j=T-i}^{T}( 1- z_j + (U_hr_j)(sech^2(U_hr_jh_{j-1})))$$\n",
        "$$\\frac{\\partial h_1}{\\partial z_1} = \\hat{h_1} = tanh(W_h)$$\n",
        "$$\\frac{\\partial z_1}{\\partial W_x} = \\sigma'(Wx)$$\n",
        "$$\\frac{\\partial L}{\\partial W_x} = (y_T - o_T)(W_y)(h_{T-1} + \\hat{h_T}) ((\\sigma'(W_xx_T + U_zh_{T-1}))$$\n",
        "$$ +\\Sigma_{i=1}^T\\Pi_{j=T-i}^{T}( 1- z_j + (U_hr_j)(sech^2(U_hr_jh_{j-1}))) (tanh(W_h)))$$\n",
        "\n",
        "It prevents the gradient from vanishing because it shuts off the gate when it is 1. \n",
        "\n",
        "**Graph for $sech^2$**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1y7xLSx9A_fE2pAbquK8qMeOj_OG1MJRa)  \n",
        "\n",
        "\n",
        "Building a text autoencoder is a little more complicated than an image autoencoder, so\n",
        "we'll need to thoroughly understand the model that we want to build before actually building\n",
        "our model. Note that the best and fastest way to complete this assignment is to spend a *lot*\n",
        "of time upfront understanding the architecture. The explanations are quite dense, and you\n",
        "might need to stop every sentence or two to understand what's going on.\n",
        "You won't feel productive for a while since you won't be writing code,\n",
        "but this initial investment will help you become more productive later on.\n",
        "Understanding this architecture will also help you understand other machine learning\n",
        "papers you might come across. So, take a deep breath, and let's do this!\n",
        "\n",
        "Here is a diagram showing our desired architecture:\n",
        "\n",
        "![](p4model.png){ width=90% }\n",
        "\n",
        "<img src=\"p4model.png\" width=\"95%\" />\n",
        "\n",
        "There are two main components to the model: the **encoder** and the **decoder**.\n",
        "As always with neural networks, we'll first describe how to make\n",
        "**predictions** with of these components. Let's get started:\n",
        "\n",
        "The **encoder** will take a sequence of words (a headline) as *input*, and produce an\n",
        "embedding (a vector) that represents the entire headline. In the diagram above,\n",
        "the vector ${\\bf h}^{(7)}$ is the vector embedding containing information about \n",
        "the entire headline.  This portion is very similar\n",
        "to the sentiment analysis RNN that we discussed in lecture (but without the fully-connected\n",
        "layer that makes a prediction).\n",
        "\n",
        "The **decoder** will take an embedding (in the diagram, the vector ${\\bf h}^{(7)}$) as input,\n",
        "and uses a separate RNN to **generate a sequence of words**. To generate a sequence of words,\n",
        "the decoder needs to do the following:\n",
        "\n",
        "1) Determine the previous word that was generated. This previous word will act as ${\\bf x}^{(t)}$\n",
        "   to our RNN, and will be used to update the hidden state ${\\bf m}^{(t)}$. Since each of our\n",
        "   sequences begin with the `<bos>` token, we'll set ${\\bf x}^{(1)}$ to be the `<bos>` token.\n",
        "2) Compute the updates to the hidden state ${\\bf m}^{(t)}$ based on the previous hidden state\n",
        "   ${\\bf m}^{(t-1)}$ and ${\\bf x}^{(t)}$. Intuitively, this hidden state vector ${\\bf m}^{(t)}$\n",
        "   is a representation of *all the words we still need to generate*.\n",
        "3) We'll use a fully-connected layer to take a hidden state ${\\bf m}^{(t)}$, and determine\n",
        "   *what the next word should be*. This fully-connected layer solves a *classification problem*,\n",
        "   since we are trying to choose a word out of $K=10000$ distinct words. As in a classification\n",
        "   problem, the fully-connected neural network will compute a *probability distribution* over\n",
        "   these 10,000 words. In the diagram, we are using ${\\bf z}^{(t)}$ to represent the logits,\n",
        "   or the pre-softmax activation values representing the probability distribution.\n",
        "4) We will need to *sample* an actual word from this probability distribution ${\\bf z}^{(t)}$.\n",
        "   We can do this in a number of ways, which we'll discuss in question 4. For now, you can \n",
        "   imagine your favourite way of picking a word given a distribution over words.\n",
        "5) This word we choose will become the next input ${\\bf x}^{(t+1)}$ to our RNN, which is used\n",
        "   to update our hidden state ${\\bf m}^{(t+1)}$---i.e. to determine what are the remaining\n",
        "   words to be generated.\n",
        "\n",
        "We can repeat this process until we see an `<eos>` token generated, or until the generated\n",
        "sequence becomes too long.\n",
        "\n",
        "Unfortunately, we can't *train* this autoencoder in the way we just described. That is,\n",
        "we can't just compare our generated sequence with our ground-truth sequence, and get\n",
        "gradients. Both sequences are **discrete** entities, so we won't be able to compute\n",
        "gradients at all! In particular, **sampling is a discrete process**, and so we won't be\n",
        "able to back-propagate through any kind of sampling that we do.\n",
        "\n",
        "You might wonder whether we can get away with computing gradients by comparing the\n",
        "distributions ${\\bf z}^{(t)}$ with the ground truth words at each time step. Like any\n",
        "multi-class classification problem, we can represent the ground-truth words as a one-hot\n",
        "vector, and use the cross-entropy loss.\n",
        "\n",
        "In theory, we can do this. In practice, there are a few issues. One is that the generated\n",
        "sequence might be longer or shorter than the actual sequence, meaning that there may\n",
        "be more/fewer ${\\bf z}^{(t)}$s than ground-truth words. Another more insidious issue\n",
        "is that the **gradients will become very high-variance and unstable**, because\n",
        "**early mistakes will easily throw the model off-track**. Early in training,\n",
        "our model is unlikely to produce the right answer in step $t=1$, so the gradients\n",
        "we obtain based on the other time steps will not be very useful.\n",
        "\n",
        "At this point, you might have some ideas about \"hacks\" we can use to make training\n",
        "work. Fortunately, there is one very well-established solution called\n",
        "**teacher forcing** which we can use for training:\n",
        "instead of *sampling* the next word based on ${\\bf z}^{(t)}$, we will forgo sampling,\n",
        "and use the **ground truth** ${\\bf x}^{(t)}$ in the next step.\n",
        "\n",
        "Here is a diagram showing how we can use **teacher forcing** to train our model:\n",
        "\n",
        "![](p4model_tf.png){ width=90% }\n",
        "\n",
        "<img src=\"p4model_tf.png\" width=\"95%\" />\n",
        "\n",
        "We will use the RNN generator to compute the logits\n",
        "${\\bf z}^{(1)},{\\bf z}^{(2)},  \\cdots {\\bf z}^{(T)}$. These distributions\n",
        "can be compared to the ground-truth words using the cross-entropy loss.\n",
        "The loss function for this model will be the sum of the losses across each $t$.\n",
        "(This is similar to what we did in a pixel-wise prediction problem.)\n",
        "\n",
        "We'll train the encoder and decoder model simultaneously. There are several components\n",
        "to our model that contain tunable weights:\n",
        "\n",
        "- The word embedding that maps a word to a vector representation.\n",
        "  In theory, we could use GloVe embeddings, or initialize our parameters to\n",
        "  GloVe embeddings. To prevent students who don't have Colab access\n",
        "  from having to download a 1GB file, we won't do that.\n",
        "  The word embedding component is represented with blue arrows in the diagram.\n",
        "- The encoder RNN (which will use Gated Recurrent Units) that computes the\n",
        "  embedding over the entire headline. The encoder RNN \n",
        "  is represented with black arrows in the diagram.\n",
        "- The decoder RNN (which will also use Gated Recurrent Units) that computes\n",
        "  hidden states, which are vectors representing what words are to be generated.\n",
        "  The decoder RNN is represented with gray arrows in the diagram.\n",
        "- The **projection MLP** (one fully-connected layer) that computes\n",
        "  a distribution over the next word to generate, given a decoder RNN hidden\n",
        "  state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdKzh0yc9IE"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        \"\"\"\n",
        "        A text autoencoder. The parameters \n",
        "            - vocab_size: number of unique words/tokens in the vocabulary\n",
        "            - emb_size: size of the word embeddings $x^{(t)}$\n",
        "            - hidden_size: size of the hidden states in both the\n",
        "                           encoder RNN ($h^{(t)}$) and the\n",
        "                           decoder RNN ($m^{(t)}$)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size, \n",
        "                                  embedding_dim=emb_size)  \n",
        "        self.encoder_rnn = nn.GRU(input_size=emb_size,\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  batch_first=True)\n",
        "        \n",
        "        #self.decoder_rnn = nn.GRU(input_size=emb_size, \n",
        "        #                          hidden_size=hidden_size,\n",
        "        #                          batch_first=True)\n",
        "        #self.proj = nn.Linear(in_features=hidden_size,\n",
        "        #                      out_features=vocab_size) \n",
        "        \n",
        "        self.decoder_rnn = nn.GRU(input_size=hidden_size, \n",
        "                                  hidden_size=emb_size, \n",
        "                                  batch_first=True)\n",
        "        self.proj = nn.Linear(in_features=emb_size, \n",
        "                              out_features=vocab_size) \n",
        "\n",
        "                              \n",
        "\n",
        "    def encode(self, inp):\n",
        "        \"\"\"\n",
        "        Computes the encoder output given a sequence of words.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.encoder_rnn(emb)\n",
        "        return last_hidden\n",
        "\n",
        "    def decode(self, inp, hidden=None):\n",
        "        \"\"\"\n",
        "        Computes the decoder output given a sequence of words, and\n",
        "        (optionally) an initial hidden state.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.decoder_rnn(emb, hidden)\n",
        "        out_seq = self.proj(out)\n",
        "        return out_seq, last_hidden\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"\n",
        "        Compute both the encoder and decoder forward pass\n",
        "        given an integer input sequence inp with shape [batch_size, seq_length],\n",
        "        with inp[a,b] representing the (index in our vocabulary of) the b-th word\n",
        "        of the a-th training example.\n",
        "\n",
        "        This function should return the logits $z^{(t)}$ in a tensor of shape\n",
        "        [batch_size, seq_length - 1, vocab_size], computed using *teaching forcing*.\n",
        "\n",
        "        The (seq_length - 1) part is not a typo. If you don't understand why\n",
        "        we need to subtract 1, refer to the teacher-forcing diagram above.\n",
        "        \"\"\"\n",
        "        \n",
        "        input = inp[:,:-1]\n",
        "        target = inp[:,1:] \n",
        "\n",
        "        hidden = self.encode(input)\n",
        "        output, hidden = self.decode(target, hidden)\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1LEvXD0c9IE"
      },
      "source": [
        "To check that your model is set up correctly, we'll train our AutoEncoder\n",
        "neural network for at least 300 iterations to memorize this sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbuYT2u-c9IE"
      },
      "source": [
        "\n",
        "headline = train_data[42].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgYlj1ADc9IF"
      },
      "source": [
        "We are looking for the way that you set up your loss function\n",
        "corresponding to the figure above.\n",
        "**Be very careful of off-by-ones.**\n",
        "\n",
        "Note that the Cross Entropy Loss expects a rank-2 tensor as its first\n",
        "argument, and a rank-1 tensor as its second argument. You will\n",
        "need to properly reshape your data to be able to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4JLhPQqc9IF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a992a4-ea2f-46a8-a3c2-ed98136eabdf"
      },
      "source": [
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "target = input_seq[:,1:] \n",
        "input = input_seq[:,:-1]\n",
        "\n",
        "hidden = 0\n",
        "\n",
        "for it in range(300):\n",
        "    optimizer.zero_grad()\n",
        "    output, hidden = model(input_seq)\n",
        "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                 target.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (it+1) % 50 == 0:\n",
        "      print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 50] Loss 0.094938\n",
            "[Iter 100] Loss 0.025564\n",
            "[Iter 150] Loss 0.016322\n",
            "[Iter 200] Loss 0.011428\n",
            "[Iter 250] Loss 0.008489\n",
            "[Iter 300] Loss 0.006606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoNIQU8vc9IF"
      },
      "source": [
        "Once you are satisfied with your model, encode your input using\n",
        "the RNN encoder, and sample some sequences from the decoder. The \n",
        "sampling code is provided to you, and performs the computation\n",
        "from the first diagram (without teacher forcing).\n",
        "\n",
        "Note that we are sampling from a multi-nomial distribution described\n",
        "by the logits $z^{(t)}$. For example, if our distribution is [80%, 20%]\n",
        "over a vocabulary of two words, then we will choose the first word\n",
        "with 80% probability and the second word with 20% probability.\n",
        "\n",
        "Call `sample_sequence` at least 5 times, with the default temperature\n",
        "value. Make sure to include the generated sequences in your PDF\n",
        "report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czuj2BT5c9IG"
      },
      "source": [
        "def sample_sequence(model, hidden, max_len=20, temperature=1):\n",
        "    \"\"\"\n",
        "    Return a sequence generated from the model's decoder\n",
        "        - model: an instance of the AutoEncoder model\n",
        "        - hidden: a hidden state (e.g. computed by the encoder)\n",
        "        - max_len: the maximum length of the generated sequence\n",
        "        - temperature: described in Part (d)\n",
        "    \"\"\"\n",
        "    # We'll store our generated sequence here\n",
        "    generated_sequence = []\n",
        "    # Set input to the <BOS> token\n",
        "    inp = torch.Tensor([text_field.vocab.stoi[\"<bos>\"]]).long()\n",
        "    for p in range(max_len):\n",
        "        # compute the output and next hidden unit\n",
        "        output, hidden = model.decode(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted word to string and use as next input\n",
        "        word = text_field.vocab.itos[top_i]\n",
        "        # Break early if we reach <eos>\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "        generated_sequence.append(word)\n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "# Your solutions go here\n",
        "print(sample_sequence(model, hidden))\n",
        "print(sample_sequence(model, hidden))\n",
        "print(sample_sequence(model, hidden))\n",
        "print(sample_sequence(model, hidden))\n",
        "print(sample_sequence(model, hidden))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0znu2kn0rMHZ"
      },
      "source": [
        "# result from previous line (pasted so it doesn't get cut off):\n",
        "#\n",
        "#['in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', \n",
        "#'in', 'in', 'in', 'in', 'in', 'in']\n",
        "#\n",
        "#['fails', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', \n",
        "#'chief', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', 'chief', \n",
        "#'chief']\n",
        "#\n",
        "#['army', 'army', 'army', 'banning']\n",
        "#\n",
        "#['decades', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', \n",
        "#'in', 'in', 'in', 'in', 'in', 'in', 'in']\n",
        "#\n",
        "#['paul', 'swears', 'swears', 'swears', 'swears', 'swears', 'swears', 'swears', 'swears', \n",
        "#'swears', 'swears', 'swears', 'swears', 'swears', 'swears', 'swears', 'swears', 'swears', \n",
        "#'swears', 'swears']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4XwUByqc9IG"
      },
      "source": [
        "The multi-nomial distribution can be manipulated using the `temperature`\n",
        "setting. This setting can be used to make the distribution \"flatter\" (e.g.\n",
        "more likely to generate different words) or \"peakier\" (e.g. less likely\n",
        "to generate different words).\n",
        "\n",
        "Call `sample_sequence` at least 5 times each for at least 3 different\n",
        "temperature settings (e.g. 1.5, 2, and 5). Explain why we generally\n",
        "don't want the temperature setting to be too **large**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "and4yu5tc9IH"
      },
      "source": [
        "# Include the generated sequences and explanation in your PDF report.\n",
        "print(sample_sequence(model, hidden, temperature=1.5))\n",
        "print(sample_sequence(model, hidden, temperature=1.5))\n",
        "print(sample_sequence(model, hidden, temperature=2.0))\n",
        "print(sample_sequence(model, hidden, temperature=2.0))\n",
        "print(sample_sequence(model, hidden, temperature=5.0))\n",
        "print(sample_sequence(model, hidden, temperature=5.0))\n",
        "\n",
        "#Increasing the tempereture, increases the noise for the multi-nomial \n",
        "#distribution data. It is good to add little randomness to cancel out\n",
        "#overfitting. However, we do not want temperature to be too large.\n",
        "#If the temperature is too large, that means the noise will be too large,\n",
        "#If the noise is large enough, the distrubition data will not be useful as \n",
        "#the random value would overwrite the training value. Therefore, we do not \n",
        "#want the temperature to be too large."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0dLqBAnq5jo"
      },
      "source": [
        "# result from previous line (pasted so it doesn't get cut off):\n",
        "#\n",
        "#['army', 'army', '_num_-vodafone', 'sight', 'golan', 'acquiring', 'punch', 'departs', \n",
        "# 'talc', 'new', 'republican', 'lights', 'chief', 'chief', 'chief', 'northwest', 'lebron', \n",
        "# 'soaring', '_num_-bristol-myers', 'peers']\n",
        "#\n",
        "#['squeezed', 'celtics', 'fourth']\n",
        "#\n",
        "#['fret', 'announced', 'abate', '_num_-hyundai', 'sweep', 'paper', 'charm', 'hangs', \n",
        "# 'jewish', 'accelerated', '_num_-britain', 'bounces', 'mou', 'ousts', 'puerto', \n",
        "# 'provincial', 'user', 'trump-xi', 'responses', 'backyard']\n",
        "#\n",
        "#['peninsula', 'swears', 'helped', 'reduced', 'budweiser', 'rep.', 'inventories', \n",
        "# 'trades', 'know', 'lockheed', 'dowdupont', 'intelligence', 'blow', 'creative', \n",
        "# 'commission', '_num_-second', 'treat', 'mobility', 'persists', 'recommendations']\n",
        "#\n",
        "#['post-election', 'km', 'virginia', 'blast', 'gunfire', 'tunnel', 'adapt', 'relation', \n",
        "# 'team', 'fisher', 'clay', 'soft', 'current-quarter', 'mps', '_num_-global', 'tries', \n",
        "# 'saved', 'mid-atlantic', 'ruling', 'dreamliner']\n",
        "#\n",
        "#['remote', 'banker', 'studies', 'pulled', 'theresa', 'firm', 'wrapup', 'imported', \n",
        "# 'snow', 'beers', 'metal', 'asean', 'sector', 'right-wing', 'sweetens', 'flight', \n",
        "# 'robert', 'erdogan', 'markets-european', 'effective']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcgGyzkFc9IH"
      },
      "source": [
        "It turns out that getting good results from a text auto-encoder is very difficult,\n",
        "and that it is very easy for our model to **overfit**. We have discussed several methods\n",
        "that we can use to prevent overfitting, and we'll introduce one more today:\n",
        "**data augmentation**.\n",
        "\n",
        "The idea behind data augmentation is to artificially increase the number of training\n",
        "examples by \"adding noise\" to the image. For example, during AlexNet training,\n",
        "the authors randomly cropped $224\\times 224$\n",
        "regions of a $256 \\times 256$ pixel image to increase the amount of training data.\n",
        "The authors also flipped the image left/right (but not up/down---why?).\n",
        "Machine learning practitioners can also add Gaussian noise to the image.\n",
        "\n",
        "When we use data augmentation to train an *autoencoder*, we typically to only add\n",
        "the noise to the input, and expect the reconstruction to be *noise free*.\n",
        "This makes the task of the autoencoder even more difficult. An autoencoder trained\n",
        "with noisy inputs is called a **denoising auto-encoder**. For simplicity, we will\n",
        "*not* build a denoising autoencoder today."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oMAa2-6c9IH"
      },
      "source": [
        "# ---Include your three answers---\n",
        "# We would not want to flip an image upside down, because the model would not really see\n",
        "# upside down images in real life. For example, there would not be upside down cats. However,\n",
        "# left-right flip images are possible and may be good variants to add to the training set.\n",
        "\n",
        "# Data Augmentation Techniques:\n",
        "# 1) Zooming on a picture. We could create new inputs by zooming on one of the inputs. This\n",
        "#   is a possible variant that the model could encounter. For example, if there is a image\n",
        "#   of a human, we could zoom in their eyes to create new data.\n",
        "# 2) Changing brightness of the image. Images brightness changes depending on the lighting.\n",
        "#   If it is night, brightness is usually lower, and if it is morning, brightness would be\n",
        "#   higher. But not all days are same, some days could be cloudy or sunny, the picture can\n",
        "#   be taken inside or outside etc. Therefore, creating new variations by changing brightness\n",
        "#   is a good way to augment the data.\n",
        "# 3) Rotating the images. The photos we take are not always perpendicular to the ground.\n",
        "#   Our photos usually have some tilt on them. We can recreate this. Rotating the image \n",
        "#   few degrees to right and left, augmenting data this way can result in new variations."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsMni_NEc9IH"
      },
      "source": [
        "We will add noise to our headlines using a few different techniques:\n",
        "\n",
        "1. Shuffle the words in the headline, taking care that words don't end up too far from where they were initially\n",
        "2. Drop (remove) some words \n",
        "3. Replace some words with a blank word (a `<pad>` token)\n",
        "4. Replace some words with a random word \n",
        "\n",
        "The code for adding these types of noise is provided for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYRGUycBc9IH"
      },
      "source": [
        "def tokenize_and_randomize(headline,\n",
        "                           drop_prob=0.1,  # probability of dropping a word\n",
        "                           blank_prob=0.1, # probability of \"blanking\" out a word\n",
        "                           sub_prob=0.1, # probability of substituting a word with a random one\n",
        "                           shuffle_dist=3): # maximum distance to shuffle a word\n",
        "    \"\"\"\n",
        "    Add 'noise' to a headline by slightly shuffling the word order,\n",
        "    dropping some words, blanking out some words (replacing with the <pad> token)\n",
        "    and substituting some words with random ones.\n",
        "    \"\"\"\n",
        "    headline = [vocab.stoi[w] for w in headline.split()]\n",
        "    n = len(headline)\n",
        "    # shuffle\n",
        "    headline = [headline[i] for i in get_shuffle_index(n, shuffle_dist)]\n",
        "\n",
        "    new_headline = [vocab.stoi['<bos>']]\n",
        "    for w in headline:\n",
        "        if random.random() < drop_prob:\n",
        "            # drop the word\n",
        "            pass\n",
        "        elif random.random() < blank_prob:\n",
        "            # replace with blank word\n",
        "            new_headline.append(vocab.stoi[\"<pad>\"])\n",
        "        elif random.random() < sub_prob:\n",
        "            # substitute word with another word\n",
        "            new_headline.append(random.randint(0, vocab_size - 1))\n",
        "        else:\n",
        "            # keep the original word\n",
        "            new_headline.append(w)\n",
        "    new_headline.append(vocab.stoi['<eos>'])\n",
        "    return new_headline\n",
        "\n",
        "def get_shuffle_index(n, max_shuffle_distance):\n",
        "    \"\"\" This is a helper function used to shuffle a headline with n words,\n",
        "    where each word is moved at most max_shuffle_distance. The function does\n",
        "    the following: \n",
        "       1. start with the *unshuffled* index of each word, which\n",
        "          is just the values [0, 1, 2, ..., n]\n",
        "       2. perturb these \"index\" values by a random floating-point value between\n",
        "          [0, max_shuffle_distance]\n",
        "       3. use the sorted position of these values as our new index\n",
        "    \"\"\"\n",
        "    index = np.arange(n)\n",
        "    perturbed_index = index + np.random.rand(n) * 3\n",
        "    new_index = sorted(enumerate(perturbed_index), key=lambda x: x[1])\n",
        "    return [index for (index, pert) in new_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tl_l9tlc9II"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_TlLBc1c9II",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba131e1-f0bb-4229-9870-a9acae293b0b"
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        "\n",
        "# I was not sure if you wanted the numbers, or the words, so i have provided both.\n",
        "\n",
        "print(\"Original Headline as Numbers:\")\n",
        "print([vocab.stoi[w] for w in train_data[42].title])\n",
        "h = ' '.join(train_data[42].title)\n",
        "print(\"\\nOriginal Headline as Words:\")\n",
        "print(h)\n",
        "\n",
        "# tokenize_and_randomize new headlines\n",
        "h1 = tokenize_and_randomize(h)\n",
        "h2 = tokenize_and_randomize(h)\n",
        "h3 = tokenize_and_randomize(h)\n",
        "h4 = tokenize_and_randomize(h)\n",
        "h5 = tokenize_and_randomize(h)\n",
        "\n",
        "print(\"\\nNew Headlines as Numbers:\")\n",
        "print(h1)\n",
        "print(h2)\n",
        "print(h3)\n",
        "print(h4)\n",
        "print(h5)\n",
        "print(\"\\nNew Headlines as Words:\")\n",
        "print(' '.join([vocab.itos[n] for n in h1]))\n",
        "print(' '.join([vocab.itos[n] for n in h2]))\n",
        "print(' '.join([vocab.itos[n] for n in h3]))\n",
        "print(' '.join([vocab.itos[n] for n in h4]))\n",
        "print(' '.join([vocab.itos[n] for n in h5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Headline as Numbers:\n",
            "[2, 5258, 91, 9117, 6, 25, 637, 118, 3]\n",
            "\n",
            "Original Headline as Words:\n",
            "<bos> zambian president swears in new army chief <eos>\n",
            "\n",
            "New Headlines as Numbers:\n",
            "[2, 1, 91, 1, 5258, 1, 7437, 637, 3, 118, 3]\n",
            "[2, 2, 5258, 9117, 4778, 4146, 637, 3, 3]\n",
            "[2, 5258, 91, 25, 1, 1, 118, 3, 3]\n",
            "[2, 5468, 5258, 91, 9117, 1, 637, 118, 3, 3]\n",
            "[2, 5258, 91, 9117, 25, 9641, 118, 637, 3, 3]\n",
            "\n",
            "New Headlines as Words:\n",
            "<bos> <pad> president <pad> zambian <pad> shoe army <eos> chief <eos>\n",
            "<bos> <bos> zambian swears handle revival army <eos> <eos>\n",
            "<bos> zambian president new <pad> <pad> chief <eos> <eos>\n",
            "<bos> uprising zambian president swears <pad> army chief <eos> <eos>\n",
            "<bos> zambian president swears new blank chief army <eos> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trKBcwpPc9IJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNODsH1Bc9IJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8100b6ea-6f01-42e4-e2c9-dca241f1d45f"
      },
      "source": [
        "def train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=10):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs):\n",
        "        # We will perform data augmentation by re-reading the input each time\n",
        "        field = torchtext.data.Field(sequential=True,\n",
        "                                     tokenize=tokenize_and_randomize, # <-- data augmentation\n",
        "                                     include_lengths=True,\n",
        "                                     batch_first=True,\n",
        "                                     use_vocab=False, # <-- the tokenization function replaces this\n",
        "                                     pad_token=vocab.stoi['<pad>'])\n",
        "        dataset = torchtext.data.TabularDataset(train_path, \"tsv\", [('title', field)])\n",
        "\n",
        "    # This BucketIterator will handle padding of sequences that are not of the same length\n",
        "        train_iter = torchtext.data.BucketIterator(dataset,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                                   repeat=False)\n",
        "        for it, ((xs, lengths), _) in enumerate(train_iter):\n",
        "            # Fill in the training code here\n",
        "            target = xs[:,1:] \n",
        "            inp = xs[:,:-1]\n",
        "            #cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, hidden = model(xs)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (it+1) % 100 == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
        "\n",
        "        # Optional: Compute and track validation loss\n",
        "        #val_loss = 0\n",
        "        #val_n = 0\n",
        "        #for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "        #    zs = model(xs)\n",
        "        #    loss = None # TODO\n",
        "        #    val_loss += float(loss)\n",
        "\n",
        "# Include your training curve or output to show that your training loss is trending down\n",
        "\n",
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "train_autoencoder(model, num_epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 100] Loss 3.077903\n",
            "[Iter 200] Loss 2.247781\n",
            "[Iter 300] Loss 1.259502\n",
            "[Iter 400] Loss 0.944643\n",
            "[Iter 500] Loss 0.791919\n",
            "[Iter 600] Loss 0.474023\n",
            "[Iter 700] Loss 0.276036\n",
            "[Iter 800] Loss 0.231561\n",
            "[Iter 900] Loss 0.140442\n",
            "[Iter 1000] Loss 0.127048\n",
            "[Iter 1100] Loss 0.055513\n",
            "[Iter 1200] Loss 0.054224\n",
            "[Iter 1300] Loss 0.034911\n",
            "[Iter 1400] Loss 0.023011\n",
            "[Iter 1500] Loss 0.017401\n",
            "[Iter 1600] Loss 0.012869\n",
            "[Iter 1700] Loss 0.010562\n",
            "[Iter 1800] Loss 0.010107\n",
            "[Iter 1900] Loss 0.009717\n",
            "[Iter 2000] Loss 0.006787\n",
            "[Iter 2100] Loss 0.007092\n",
            "[Iter 2200] Loss 0.005752\n",
            "[Iter 2300] Loss 0.005043\n",
            "[Iter 2400] Loss 0.003855\n",
            "[Iter 2500] Loss 0.003555\n",
            "[Iter 2600] Loss 0.003616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ww0d3ac9IJ"
      },
      "source": [
        "This model requires many epochs (>50) to train, and is quite slow without using a GPU.\n",
        "\n",
        "Assuming that your `AutoEncoder` is set up correctly, the following code should run without\n",
        "error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-WglLcCc9IK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17bb1eb-c3fe-4259-e002-428730f7d6bc"
      },
      "source": [
        "model = AutoEncoder(10000, 128, 128)\n",
        "checkpoint_path = '/content/gdrive/My Drive/p4model.pk' # Update me\n",
        "model.load_state_dict(torch.load(checkpoint_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_xI453Tc9IK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXviy5jMc9IK"
      },
      "source": [
        "# Include the generated sequences and explanation in your PDF report.\n",
        "\n",
        "headline = train_data[10].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).unsqueeze(0).long()\n",
        "\n",
        "print(sample_sequence(model, None, temperature=0.7))\n",
        "print(sample_sequence(model, None, temperature=0.7))\n",
        "print(sample_sequence(model, None, temperature=0.9))\n",
        "print(sample_sequence(model, None, temperature=0.9))\n",
        "print(sample_sequence(model, None, temperature=1.5))\n",
        "print(sample_sequence(model, None, temperature=1.5))\n",
        "\n",
        "# If the temperature setting is too small, it is similar to overfitting. If there is\n",
        "# not enough randomness, the model will create the very similar tweets. We want our model\n",
        "# to create different stuff, try different solutions. Therefore, the temperature setting\n",
        "# needs to be bigger. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Avten28XrNn"
      },
      "source": [
        "# result from previous line (pasted so it doesn't get cut off):\n",
        "#\n",
        "#['heartland', 'manning', 'survival', 'deaths', '<pad>', '<pad>', '<pad>', '<pad>', \n",
        "# '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'jobs', '<pad>', \n",
        "# '<pad>', '<pad>']\n",
        "#\n",
        "#['facial', 'sempra', 'endorse', '<pad>']\n",
        "#\n",
        "#['tpg', 'brf', '+']\n",
        "#\n",
        "#['tycoon', 's.african', 'surges', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \n",
        "# '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
        "# '<pad>']\n",
        "#\n",
        "#['caltex', 'sending', 'recorded', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', \n",
        "# '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'takes', '<pad>', '<pad>',\n",
        "# 'discussions']\n",
        "#\n",
        "#['restoring', 'pats', 'daily']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn3_3ZfPc9IK"
      },
      "source": [
        "In parts 2-3, we've explored the decoder portion of the autoencoder. In this section,\n",
        "let's explore the **encoder**. In particular, the encoder RNN gives us \n",
        "embeddings of news headlines!\n",
        "\n",
        "First, let's load the **validation** data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXo0eD2uc9IK"
      },
      "source": [
        "valid_data = torchtext.data.TabularDataset(\n",
        "    path=valid_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (we have only one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sczgs7_8c9IK"
      },
      "source": [
        "Compute the embeddings of every item in the validation set. Then, store the\n",
        "result in a single PyTorch tensor of shape `[19046, 128]`, since there are\n",
        "19,046 headlines in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uERipRy3c9IK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86648f22-156a-4b2d-a992-0d06a807fde9"
      },
      "source": [
        "# Write your code here\n",
        "# Show that your resulting PyTorch tensor has shape `[19046, 128]`\n",
        "\n",
        "embedding_values = []\n",
        "for data in valid_data:\n",
        "    headline = data.title\n",
        "    input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)\n",
        "    target = input_seq[:,1:] \n",
        "    input = input_seq[:,:-1]\n",
        "    embedding = model.encode(input)\n",
        "    embedding_values.append(embedding)\n",
        "embedding_values = torch.stack(embedding_values)\n",
        "# it has shape [19046, 1, 1, 128]\n",
        "embedding_values = embedding_values.reshape([19046, 128]) \n",
        "\n",
        "print(embedding_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19046, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHYM5VKKc9IL"
      },
      "source": [
        "Find the 5 closest headlines to the headline `valid_data[13]`. Use the\n",
        "cosine similarity to determine closeness. (Hint: You can use code from Project 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI_9qaoKc9IL"
      },
      "source": [
        "# Write your code here. Make sure to include the actual 5 closest headlines.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def cosinesimilarity(emb1, emb2):\n",
        "  s = tf.keras.losses.cosine_similarity(emb1.detach().numpy(), emb2.detach().numpy())\n",
        "  return 1-s\n",
        "\n",
        "\n",
        "headline = valid_data[13].title\n",
        "e0 = embedding_values[13]\n",
        "\n",
        "print(\"Original Headline\")\n",
        "print(\"Valid Data 13\")\n",
        "print(headline)\n",
        "\n",
        "\n",
        "similarities = []\n",
        "for i in range(len(valid_data)):\n",
        "  if i == 13:\n",
        "    similarities.append(0)\n",
        "  else:\n",
        "    e1 = embedding_values[i]\n",
        "    s = cosinesimilarity(e0,e1).numpy()\n",
        "    similarities.append(s)\n",
        "\n",
        "temp = similarities.copy()\n",
        "temp.sort()\n",
        "\n",
        "print(\"\\n1st Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-1]))\n",
        "print(valid_data[similarities.index(temp[-1])].title)\n",
        "print(\"\\n2nd Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-2]))\n",
        "print(valid_data[similarities.index(temp[-2])].title)\n",
        "print(\"\\n3rd Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-3]))\n",
        "print(valid_data[similarities.index(temp[-3])].title)\n",
        "print(\"\\n4th Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-4]))\n",
        "print(valid_data[similarities.index(temp[-4])].title)\n",
        "print(\"\\n5th Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-5]))\n",
        "print(valid_data[similarities.index(temp[-5])].title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRCPrThzZIwQ"
      },
      "source": [
        "# result from previous line (pasted so it doesn't get cut off):\n",
        "#\n",
        "#Original Headline\n",
        "#Valid Data 13\n",
        "#['<bos>', 'asia', 'takes', 'heart', 'from', 'new', 'year', 'gains', 'in', 'u.s.', 'stock', \n",
        "# 'futures', '<eos>']\n",
        "#\n",
        "#1st Closest Headline\n",
        "#Valid Data 2879\n",
        "#['<bos>', 'india', \"'s\", 'palm', 'oil', 'imports', 'could', 'jump', 'to', 'record', 'as', \n",
        "#'prices', 'fall', '-', 'analyst', '<eos>']\n",
        "#\n",
        "#2nd Closest Headline\n",
        "#Valid Data 13183\n",
        "#['<bos>', 'factbox', ':', 'who', \"'s\", 'who', 'in', 'spain', \"'s\", 'snap', 'parliamentary',\n",
        "# 'election', '<eos>']\n",
        "#\n",
        "#3rd Closest Headline\n",
        "#Valid Data 13808\n",
        "#['<bos>', 'new', 'zealand', 'dollar', 'set', 'for', 'biggest', 'jump', 'this', 'year', \n",
        "#'on', 'central', 'bank', 'surprise', '<eos>']\n",
        "#\n",
        "#4th Closest Headline\n",
        "#Valid Data 14055\n",
        "#['<bos>', 'test', 'for', 'ethiopia', \"'s\", 'reforms', 'as', 'sidama', 'people', 'vote', \n",
        "#'on', 'autonomy', '<eos>']\n",
        "#\n",
        "#5th Closest Headline\n",
        "#Valid Data 3990\n",
        "#['<bos>', 'india', \"'s\", 'polarized', 'politics', ':', 'how', 'two', 'teenagers', 'will',\n",
        "# 'vote', 'after', 'surviving', 'riots', '<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bj36-Tdc9IL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Ua62hwZWzG"
      },
      "source": [
        "headline = valid_data[10].title\n",
        "e0 = embedding_values[10]\n",
        "\n",
        "print(\"Original Headline\")\n",
        "print(\"Valid Data 10\")\n",
        "print(headline)\n",
        "\n",
        "\n",
        "similarities = []\n",
        "for i in range(len(valid_data)):\n",
        "  if i == 10:\n",
        "    similarities.append(0)\n",
        "  else:\n",
        "    e1 = embedding_values[i]\n",
        "    s = cosinesimilarity(e0,e1).numpy()\n",
        "    similarities.append(s)\n",
        "\n",
        "temp = similarities.copy()\n",
        "temp.sort()\n",
        "\n",
        "print(\"\\n1st Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-1]))\n",
        "print(valid_data[similarities.index(temp[-1])].title)\n",
        "print(\"\\n2nd Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-2]))\n",
        "print(valid_data[similarities.index(temp[-2])].title)\n",
        "print(\"\\n3rd Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-3]))\n",
        "print(valid_data[similarities.index(temp[-3])].title)\n",
        "print(\"\\n4th Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-4]))\n",
        "print(valid_data[similarities.index(temp[-4])].title)\n",
        "print(\"\\n5th Closest Headline\")\n",
        "print(\"Valid Data\",similarities.index(temp[-5]))\n",
        "print(valid_data[similarities.index(temp[-5])].title)\n",
        "\n",
        "\n",
        "# result from previous line (pasted so it doesn't get cut off):\n",
        "#\n",
        "#Original Headline\n",
        "#Valid Data 10\n",
        "#['<bos>', 'trump', 'invites', 'congressional', 'leaders', 'to', 'border', 'security', \n",
        "# 'briefing', '-', 'source', '<eos>']\n",
        "#\n",
        "#1st Closest Headline\n",
        "#Valid Data 11417\n",
        "#['<bos>', 'trump', 'told', 'russians', 'in', '_num_', 'not', 'concerned', 'by', \n",
        "# 'meddling', '-wapo', '<eos>']\n",
        "#\n",
        "#2nd Closest Headline\n",
        "#Valid Data 15455\n",
        "#['<bos>', 'pm', 'johnson', 'bans', 'ministers', 'from', 'attending', 'davos', '-',\n",
        "# 'source', '<eos>']\n",
        "#\n",
        "#3rd Closest Headline\n",
        "#Valid Data 14713\n",
        "#['<bos>', 'italy', 'set', 'to', 'grant', 'funds', 'to', 'keep', 'alitalia', 'afloat',\n",
        "# '-', 'source', '<eos>']\n",
        "#\n",
        "#4th Closest Headline\n",
        "#Valid Data 638\n",
        "#['<bos>', 'u.s.', 'asked', 'ecuadorean', 'officials', 'about', 'alleged',\n",
        "# 'assange-manafort', 'meeting', '-', 'source', '<eos>']\n",
        "#\n",
        "#5th Closest Headline\n",
        "#Valid Data 14983\n",
        "#['<bos>', 'macron', 'ally', 'treated', 'as', 'suspect', 'in', 'financial', 'impropriety',\n",
        "# 'case', ':', 'source', '<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swn3VH2Pc9IM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqieB0yCc9IN"
      },
      "source": [
        "# Write your code here. Include your generated sequences.\n",
        "# Your solutions go here\n",
        "\n",
        "def decode(model, inp, hidden, temperature=1):\n",
        "    \"\"\"\n",
        "    Return a sequence generated from the model's decoder\n",
        "        - model: an instance of the AutoEncoder model\n",
        "        - hidden: a hidden state (e.g. computed by the encoder)\n",
        "        - max_len: the maximum length of the generated sequence\n",
        "        - temperature: described in Part (d)\n",
        "    \"\"\"\n",
        "    # We'll store our generated sequence here\n",
        "    generated_sequence = []\n",
        "    # Set input to the <BOS> token\n",
        "    for i in range(inp.shape[1]):\n",
        "        # compute the output and next hidden unit\n",
        "        input = inp[:,i,:].reshape([inp.shape[0],1,inp.shape[2],])\n",
        "        output, _ = model.decoder_rnn(input, hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted word to string and use as next input\n",
        "        word = text_field.vocab.itos[top_i]\n",
        "        # Break early if we reach <eos>\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "        generated_sequence.append(word)\n",
        "    return generated_sequence\n",
        "\n",
        "\n",
        "\n",
        "headline1 = valid_data[11].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline1]).long().unsqueeze(0)\n",
        "e0 = model.embed(input_seq)\n",
        "\n",
        "headline2 = valid_data[13].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline2]).long().unsqueeze(0)\n",
        "e4 = model.embed(input_seq)\n",
        "\n",
        "\n",
        "e1 = e0 * (1/4) + e4 * (3/4)\n",
        "e2 = e0 * (2/4) + e4 * (2/4)\n",
        "e3 = e0 * (3/4) + e4 * (1/4)\n",
        "\n",
        "print(\"E0\")\n",
        "print(headline1)\n",
        "\n",
        "\n",
        "print(\"\\nE1 5 variations\")\n",
        "print(decode(model, e1, hidden, temperature = 0.8))\n",
        "print(decode(model, e1, hidden, temperature = 1.0))\n",
        "print(decode(model, e1, hidden, temperature = 1.5))\n",
        "print(decode(model, e1, hidden, temperature = 2.0))\n",
        "print(decode(model, e1, hidden, temperature = 5.0))\n",
        "\n",
        "print(\"\\nE2 5 variations\")\n",
        "print(decode(model, e2, hidden, temperature = 0.8))\n",
        "print(decode(model, e2, hidden, temperature = 1.0))\n",
        "print(decode(model, e2, hidden, temperature = 1.5))\n",
        "print(decode(model, e2, hidden, temperature = 2.0))\n",
        "print(decode(model, e2, hidden, temperature = 5.0))\n",
        "\n",
        "print(\"\\nE3 5 variations\")\n",
        "print(decode(model, e3, hidden, temperature = 0.8))\n",
        "print(decode(model, e3, hidden, temperature = 1.0))\n",
        "print(decode(model, e3, hidden, temperature = 1.5))\n",
        "print(decode(model, e3, hidden, temperature = 2.0))\n",
        "print(decode(model, e3, hidden, temperature = 5.0))\n",
        "\n",
        "print(\"\\nE4\")\n",
        "print(headline2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHz1VKsKZil4"
      },
      "source": [
        "# result from previous line (pasted so it doesn't get cut off):\n",
        "#\n",
        "#E0\n",
        "#['<bos>', 'uk', 'police', 'detain', 'young', 'man', 'in', 'manchester', 'stabbing', ',',\n",
        "# 'investigation', 'continues', '<eos>']\n",
        "#\n",
        "#E1 5 variations\n",
        "#['first', 'but', 'brexit', 'court', 'more', 'first', 'brexit', 'to', 'more', 'sources',\n",
        "# 'government', 'official', 'india']\n",
        "#['top', 'be', 'house', 'brexit', 'of', 'bln', 'at', \"'s\", 'court', ';', 'boeing', 'of',\n",
        "# 'it']\n",
        "#['new', 'and', 'is', 'but', 'as', 'could', 'two', 'court', 'brexit', 'global', '&',\n",
        "# 'demand', 'growth']\n",
        "#['by', 'a', 'shares', 'billion', 'us', 'as', 'as', 'japan', 'ahead', 'saudi', 'after',\n",
        "# 'iran', 'will']\n",
        "#['house', 'by', \"'s\", '&', 'fall', 'election', 'business', '<pad>', 'over', 'off',\n",
        "# 'police', 'mexico', 'north']\n",
        "#\n",
        "#E2 5 variations\n",
        "#['against', 'coronavirus', 'brazil', 'and', 'up', 'at', 'government', 'new', 'pm',\n",
        "# 'south', 'bank', 'election', 'election']\n",
        "#['hong', 'data', 'trump', 'open', 'against', '<unk>', 'official', 'prices', 'cuts',\n",
        "# 'a', 'chief', \"'s\", 'out']\n",
        "#['off', 'fed', 'against', 'wall', 'be', 'no', 'off', 'brexit', 'global', 'be', 'court',\n",
        "# 'down', 'may']\n",
        "#['official', 'fall', 'china', 'at', 'billion', 'deal']\n",
        "#['back', 'against']\n",
        "#\n",
        "#E3 5 variations\n",
        "#['world', 'more', 'oil', 'new', 'growth', 'after', 'cuts', 'korea', 'coronavirus',\n",
        "# 'house', 'coronavirus', 'it', 'cuts']\n",
        "#['ceo', 'minister', 'in', ';', 'state', 'ceo', \"'s\", 'government', 'wall', 'off']\n",
        "#['and', '<pad>', '<bos>', 'of', 'may', 'korea', 'india', 'as', 'canada', 'shares', 'it',\n",
        "# 'two', 'japan']\n",
        "#['but', ';', 'japan', ':', 'million', 'no', 'amid', 'iran', 'minister', 'hit', 'boeing',\n",
        "# 'south', ';']\n",
        "#['on', 'could', 'high', 'police', 'president', 'be', 'north', '<bos>', 'japan', 'u.s.',\n",
        "# 'north', 'india', 'kong']\n",
        "#\n",
        "#E4\n",
        "#['<bos>', 'asia', 'takes', 'heart', 'from', 'new', 'year', 'gains', 'in', 'u.s.',\n",
        "# 'stock', 'futures', '<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}